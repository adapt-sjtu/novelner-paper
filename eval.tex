\section{Experiments}
\label{sec:eval}

\subsection{Parameter Initialization}
For word-level word representation (i.e. the lookup table), 
we utilize the pretrained word embeddings from glove\footnote{\url{http://nlp.stanford.edu/data/glove.twitter.27B.zip}}.
For all out-of-vocabulary words, we assign their embeddings by randomly sampling from range $\left[-\sqrt{\frac{3}{\text{dim}}}, +\sqrt{\frac{3}{\text{dim}}}~\right]$, where \textit{dim} is the dimension of word embeddings, suggested by He et al.(\citeyear{DBLP:conf/iccv/HeZRS15}). The random initialization of character embeddings are in the same way.
We randomly initialize the weight matrices $\mathbf{W}$ and $\mathbf{b}$ with uniform samples from 
$\left[-\sqrt{\frac{6}{r+c}}, +\sqrt{\frac{6}{r+c}}~\right]$, 
$r$ and $c$ are the number of the rows and columns, following Glorot and Bengio(\citeyear{DBLP:journals/jmlr/GlorotB10}), and all LSTM hidden states are initialized to be zero except for the bias for the forget gate is initialized to be 1.0 , following Jozefowicz et al.(\citeyear{DBLP:conf/icml/JozefowiczZS15}) 


\subsection{Hyper Parameter Tuning}
We tuned the dimension of word-level embeddings from \{50,\textbf{100},200\}, character embeddings from \{10,\textbf{25},50\}, character BiLSTM hidden states (i.e. the character level word representation)  from  \{20,\textbf{50},100\}. 
We finally choose the bold ones.
The dimension of POS Tag, Dependecny Roles, position and \BL{head} are all 5.

As for learning method, we compare the traditional SGD and Adam.
We found that Adam performs always better than SGD, and we tune the learning rate form \{1e-2,\textbf{1e-3},1e-4\}.

\subsection{Results} 
In comparison with other participants, the results are shown in 


Team	F1 (entity)	F1 (surface form)
MIC-CIS	36.90	50.38
Arcada	40.09	56.60
Drexel-CCI	26.81	59.92
SJTU-Adapt	41.22	60.00
FLYTXT	38.10	57.64
SpinningBytes	41.76	57.98
UH-RiTUAL	41.90	66.59

\BL{jessie plz add a table here }
