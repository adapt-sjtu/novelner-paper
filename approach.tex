\section{Approach}
\label{sec:approach}
In this section, we will first introduce the overview of our proposed model and then present each part of the model in detail.

\subsection{Overview}
\figref{fig:overall} shows the overall structure of our proposed model, 
instead of solely using the original pretrained word embeddings as the final word representations, 
we construct a comprehensive word representation for each word in the input sentence.
This comprehensive word representations contain the character-level sub-word information, the original pretrained word embeddings and multiple syntactical features. 
Then, we feed them into a Bidirectional LSTM layer, and thus we have a hidden state for each word. 
The hidden states are considered as the feature vectors of the words by the final CRF layer, from which we can decode the final predicted tag sequence for the input sentence.

\subsection{Comprehensive Word Representations}
In this subsection, we present our proposed comprehensive word representations. 
We first build character-level word representations from the embeddings of every characters in each word using a bidirectional LSTM. 
Then we further incorporate the final word representation with the embedding of the syntactical information of each token, such as the part-of-speech tag, the dependency role, the position in the sentence and the position of the head. \BL{i am not sure about the HEAD. maybe we can find a better description.}
Finally, we combine the original word embeddings with the above two parts to obtain the final comprehensive word representations.

\subsubsection{Character-level Word Representations}
In noisy user-generated text analysis, sub-word (character-level) information is much more important than that in normal text analysis for two main reasons:
1) People are more likely to use novel abbreviations and morphs to mention entities, which are often out of vocabulary and only occur a few times. 
Thus, solely using the original word-level word embedding as features to represent  words is not adequate to capture the characteristics of such mentions.
2) Another reason why we have to pay more attention to character-level word representation for noisy text is that it is can capture the orthographic or morphological information of both formal words and Internet slang.
\footnote{\BL{the abbr. examples and morph stuff should be mentioned in the introduction part as one of the reasons why twitter NER is hard and cite Heng Ji's paper}} 

There are two main network structures to make use of character embeddings: 
one is CNN (Ma and Hovy, 2016 \cite{} ) and the other is BiLSTM(Lample et al. 2016).
BiLSTM turns to be better in our experiment on development dataset and we will discuss the reason in \secref{sec:eval}.
Thus, we follow Lample et al. (2016) to build a BiLSTM network to encode the characters in each token as \figref{arg1} shows. \BL{see Figure 4 in Lample's paper.}
We finally concatenate the forward embedding and backward embedding to the final character-level word representation.
%such as ``kktny'' meaning \textit{Kourtney and Kim Take New York}, ``himym'' meaning ``How I Met Your Mother''. 
%Also, people create new and emerging morphs for entities for fun or avoiding censorship, which a


%http://ac.els-cdn.com/S1877050917306580/1-s2.0-S1877050917306580-main.pdf?_tid=139e4e02-617c-11e7-9a7c-00000aab0f27&acdnat=1499257299_dd0b70d56f16720625b3ac5eec4d7284

\subsubsection{Syntactical Word Representations}
We argue that the syntactical information, such as POS tag and dependency role, should also be explicitly considered as context features of each token in the sentence. 
\BL{Jessie plz introduce how we extract the twitter pos tags etc. from the Tweebo parser. Better show an example }.
 
\subsubsection{Combination with Word-level Word Representations}
After obtaining the above two additional word representations, we combine them with the original word-level word representations, which are just traditional word embeddings. 
We utilize pretrained word embeddings from glove\footnote{\url{http://nlp.stanford.edu/data/glove.twitter.27B.zip}}.
For all out-of-vocabulary words, we assign their embeddings by sampling from range $\left[-\sqrt{\frac{3}{\text{dim}}}, +\sqrt{\frac{3}{\text{dim}}}\right]$, where \textit{dim} is the dimension of word embeddings.

To sum up, our comprehensive word representations are the concatenation of three parts: 1) character-level word representations, 2) syntactical word representation and 3)  original pretrained word embeddings.

\subsection{BiLSTM Layer}
LSTM based networks are proven to be effective in sequence labeling problem for they have access to both past and the future contexts. 
Whereas, hidden states in unidirectional LSTMs only takes information from the past, which may be adequate to classify the sentiment  is a shortcoming for labeling each token.
Bidirectional LSTMs enable the hidden states to capture both historical and future context information and then to label a token.

Mathematically, the input of this BiLSTM layer is a sequence of comprehensive word representations (vectors) for the tokens of the input sentence,  denoted as 
$( \mathbf{x_1}, \mathbf{x_2},...,\mathbf{x_n})$. 
The output of this BiLSTM layer is a sequence of the hidden states for each input word vectors, denoted as 
$( \mathbf{h_1}, \mathbf{h_2},...,\mathbf{h_n})$. 
Each final hidden state is the concatenation of the forward $\overleftarrow{\mathbf{h_i}}$ and backward $\overrightarrow{\mathbf{h_i}}$ hidden states.
We know that 

$$\overleftarrow{\mathbf{h_i}}= \text{lstm}(\mathbf{x_i}, \overleftarrow{\mathbf{h_{i-1}}})~\text{,}~\overrightarrow{\mathbf{h_i}}= \text{lstm}(\mathbf{x_i}, \overrightarrow{\mathbf{h_{i+1}}})$$ 

$$\mathbf{h_i} = \left[ \overleftarrow{\mathbf{h_i}} ; \overrightarrow{\mathbf{h_i}} \right]$$


\subsection{CRF Layer}
It is almost always beneficial to consider the correlations between two between current label and neighboring labels since there are many syntactical constrains in natural language sentences. 
For example, I-PERSON will never follow a B-GROUP. 
If we simply feed the above mentioned hidden states independently to a Softmax layer to predict the labels, then such constrains will not be more likely to be broken. 
Linear-chain Conditional Random Field is the most popular way to control the structure prediction and its basic idea is to use a series of potential function to approximate the conditional probability of the output label sequence given the input word sequence. 

Formally, we take the above sequence of hidden states  $ \mathbf{h} = ( \mathbf{h_1}, \mathbf{h_2},...,\mathbf{h_n})$ as our input to the CRF layer, and its output is our final prediction label sequence $\mathbf{y} = ( {y_1}, {y_2},...,{y_n})$, where $y_i$ is in the set of all possible labels. 
We denote $\mathcal{Y}(\mathbf{h})$ as the set of all possible label sequences.
Then we derive the conditional probability of the output sequence given the input hidden state sequence is 

{\footnotesize $$ p(\mathbf{y}|\mathbf{h}; \mathbf{W},\mathbf{b}) 
= \frac{\prod_{i=1}^n \exp(\mathbf{W}^T_{y_{i-1},y_{i}}\mathbf{h} + \mathbf{b}_{y_{i-1},y_{i}})}
{ \sum_{\mathbf{y'} \in \mathcal{Y}(\mathbf{h})} \prod_{i=1}^n \exp(\mathbf{W}^T_{y'_{i-1},y'_{i}}\mathbf{h} + \mathbf{b}_{y'_{i-1},y'_{i}})} 
$$
}

, where $\mathbf W$ and $\mathbf b$ are the two weight matrices and the subscription indicates that we extract the weight vector for the given label pair $(y_{i-1},y_i)$.

To train the CRF layer, we use the classic maximum conditional likelihood estimation to train our model. 
The final log-likelihood with respect to the weight matrices is 

$$ L(\mathbf{W},\mathbf{b}) = \sum_{(\mathbf{h_i}, \mathbf{y_i})}  \log p(\mathbf{y_i}|\mathbf{h_i}; \mathbf{W},\mathbf{b}) $$


Finally, we adopt the Viterbi algorithm for training the CRF layer and the decoding the optimal output sequence $\mathbf{y^*}$.







